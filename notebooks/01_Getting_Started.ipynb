{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 01 - Getting Started\n",
    "\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Triton Inference Server](#triton)\n",
    "* [Setup](#setup)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will walk through what Triton Inference Server is as well as do some light setup for our lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"triton\"></a>\n",
    "### Triton Inference Server\n",
    "\n",
    "NVIDIA Triton Inference Server simplifies the deployment of AI models at scale in production. Triton is an open-source, inference-serving software that lets teams deploy trained AI models from any framework, from local storage, or from Google Cloud Platform or Azure on any GPU or CPU-based infrastructure, cloud, data center, or edge. One can get started with Triton by pulling the container from the NVIDIA NGC catalog, the hub for GPU-optimized software for deep learning and machine learning that accelerates deployment to development workflows.\n",
    "\n",
    "The below figure shows the Triton Inference Server high-level architecture. The model repository is a file-system based repository of the models that Triton will make available for inferencing. Inference requests arrive at the server via either HTTP/REST or GRPC or by the C API and are then routed to the appropriate per-model scheduler. Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis. Each model's scheduler optionally performs batching of inference requests and then passes the requests to the backend corresponding to the model type. The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs. The outputs are then returned.\n",
    "\n",
    "<img src=\"./assets/A-schematic-of-Triton-Server-architecture.png\" alt=\"A Schematic of Triton Inference Server\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "### Setup\n",
    "\n",
    "First, let's check what GPUs we have on our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 31 17:21:49 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.04              Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX               On  | 00000000:02:00.0  On |                  N/A |\n",
      "| 41%   37C    P8              22W / 280W |    904MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our system has 1 GPU, a Tesla T4.\n",
    "\n",
    "Additionally, let's examine our file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 160K\n",
      "drwxrwxrwx 1 renan renan  512 Oct 31 13:05 .\n",
      "drwxrwxrwx 1 renan renan  512 Oct 31 16:06 ..\n",
      "drwxrwxrwx 1 renan renan  512 Oct 31 13:03 .ipynb_checkpoints\n",
      "-rwxrwxrwx 1 renan renan 4.4K Dec 14  2021 00_jupyterlab.ipynb\n",
      "-rwxrwxrwx 1 renan renan 6.7K Dec 14  2021 01_Getting_Started.ipynb\n",
      "-rwxrwxrwx 1 renan renan  28K Oct 31 13:05 02_Simple_PyTorch_Model.ipynb\n",
      "-rwxrwxrwx 1 renan renan  15K Dec 14  2021 03_HuggingFace_NLP_Model.ipynb\n",
      "-rwxrwxrwx 1 renan renan  17K Dec 14  2021 04_Simple_TensorFlow_Model.ipynb\n",
      "-rwxrwxrwx 1 renan renan  23K Dec 14  2021 05_Simple_TensorRT_Model.ipynb\n",
      "-rwxrwxrwx 1 renan renan  26K Nov 24  2021 06_Advanced_Inference.ipynb\n",
      "-rwxrwxrwx 1 renan renan 8.5K Dec 14  2021 07_Metrics.ipynb\n",
      "drwxrwxrwx 1 renan renan  512 Oct 30 21:12 assets\n",
      "-rwxrwxrwx 1 renan renan  14K Nov 18  2021 imagenet-simple-labels.json\n",
      "drwxrwxrwx 1 renan renan  512 Oct 28 21:26 models\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several folders and Jupyter notebooks. We'll visit each of these notebooks later in the lab. Lastly, let's check which version of CUDA we're working with. We can see from the output below that we're working with CUDA 11.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Sep__8_19:17:24_PDT_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.52\n",
      "Build cuda_12.3.r12.3/compiler.33281558_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Server**\n",
    "\n",
    "In this lab, we already have Triton Inference Server instance running. The code to run a Triton Server Instance is shown below. More details can be found in the quickstart and build instructions:\n",
    "\n",
    "* [Quickstart Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/quickstart.md)\n",
    "* [Build Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/build.md)\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "  --gpus=1 \\\n",
    "  --ipc=host --rm \\\n",
    "  --shm-size=1g \\\n",
    "  --ulimit memlock=-1 \\\n",
    "  --ulimit stack=67108864 \\\n",
    "  -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v /models:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:20.12-py3 \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --model-control-mode=poll \\\n",
    "  --repository-poll-secs 30\n",
    "```\n",
    "\n",
    "Triton Inference Server container can be found on NGC: https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver\n",
    "\n",
    "**Client**\n",
    "\n",
    "We've also installed the Triton Inference Server Client libraries that provide APIs that make it easy to communicate with Triton from your C++ or Python application. Using these libraries you can send either HTTP/REST or GRPC requests to Triton to access all its capabilities: inferencing, status and health, statistics and metrics, model repository management, etc. These libraries also support using system and CUDA shared memory for passing inputs to and receiving outputs from Triton. Examples show the use of both the C++ and Python libraries.\n",
    "\n",
    "The easiest way to get the Python client library is to use pip to install the `tritonclient` module, as detailed below. For more details on how to download or build the Triton Inference Server Client libraries, you can find the documentation here: https://github.com/triton-inference-server/server/blob/r20.12/docs/client_libraries.md\n",
    "\n",
    "```\n",
    "pip install nvidia-pyindex\n",
    "pip install tritonclient[all]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we walked through what Triton Inference Server is as well as did some light setup for our lab. Feel free to move onto the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
